{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Setting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Fine-tuning LLaMA using LoRA\n",
    "\n",
    "# Loading LLaMA tokenizer and model\n",
    "llama_model_name = \"huggyllama/llama-7b\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_model_name).to(device)\n",
    "\n",
    "# Loading dataset for LLaMA (using wikitext as an example)\n",
    "llama_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Tokenizing the dataset\n",
    "def tokenize_function_llama(examples):\n",
    "    return llama_tokenizer(examples['text'], return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "llama_dataset = llama_dataset.map(tokenize_function_llama, batched=True)\n",
    "\n",
    "# LoRA configuration for LLaMA\n",
    "llama_lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Applying LoRA to LLaMA model\n",
    "llama_peft_model = get_peft_model(llama_model, llama_lora_config)\n",
    "\n",
    "# Training arguments for LLaMA\n",
    "llama_training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_lora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer for LLaMA\n",
    "llama_trainer = Trainer(\n",
    "    model=llama_peft_model,\n",
    "    args=llama_training_args,\n",
    "    train_dataset=llama_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tuning LLaMA\n",
    "llama_trainer.train()\n",
    "\n",
    "### Fine-tuning BERT using LoRA\n",
    "\n",
    "# Loading BERT tokenizer and model\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=2).to(device)\n",
    "\n",
    "# Loading dataset for BERT (using IMDB movie reviews as an example)\n",
    "bert_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Tokenizing the dataset\n",
    "def tokenize_function_bert(examples):\n",
    "    return bert_tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "bert_encoded_dataset = bert_dataset.map(tokenize_function_bert, batched=True)\n",
    "\n",
    "# LoRA configuration for BERT\n",
    "bert_lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query\", \"key\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# Applying LoRA to BERT model\n",
    "bert_peft_model = get_peft_model(bert_model, bert_lora_config)\n",
    "\n",
    "# Training arguments for BERT\n",
    "bert_training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_lora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer for BERT\n",
    "bert_trainer = Trainer(\n",
    "    model=bert_peft_model,\n",
    "    args=bert_training_args,\n",
    "    train_dataset=bert_encoded_dataset[\"train\"],\n",
    "    eval_dataset=bert_encoded_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# Fine-tuning BERT\n",
    "bert_trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
